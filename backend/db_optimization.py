"""
ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
- ì¸ë±ìŠ¤ ìƒì„±
- ì¿¼ë¦¬ ìµœì í™”
- ì—°ê²° í’€ë§
- ë°°ì¹˜ ì²˜ë¦¬
"""

import sqlite3
from contextlib import contextmanager
import time
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class DatabaseOptimizer:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.optimization_results = []
    
    @contextmanager
    def get_connection(self):
        """ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        conn.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging
        conn.execute("PRAGMA synchronous = NORMAL")  # ë” ë¹ ë¥¸ ì“°ê¸°
        conn.execute("PRAGMA cache_size = -64000")  # 64MB ìºì‹œ
        conn.execute("PRAGMA temp_store = MEMORY")  # ì„ì‹œ í…Œì´ë¸”ì„ ë©”ëª¨ë¦¬ì—
        conn.execute("PRAGMA mmap_size = 268435456")  # 256MB ë©”ëª¨ë¦¬ ë§µ
        
        try:
            yield conn
        finally:
            conn.close()
    
    def create_indexes(self):
        """í•„ìš”í•œ ì¸ë±ìŠ¤ ìƒì„±"""
        print("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™” ì‹œì‘...")
        
        indexes = [
            # ì‚¬ìš©ì ê´€ë ¨ ì¸ë±ìŠ¤
            ("idx_users_email", "users(email)"),
            ("idx_users_username", "users(username)"),
            
            # í‚¤ì›Œë“œ ê´€ë ¨ ì¸ë±ìŠ¤
            ("idx_keywords_user_created", "keywords(user_id, created_at DESC)"),
            ("idx_keywords_search_volume", "keywords(search_volume DESC)"),
            
            # ìƒì„±ëœ ì½˜í…ì¸  ì¸ë±ìŠ¤
            ("idx_generated_content_user_created", "generated_content(user_id, created_at DESC)"),
            ("idx_generated_content_site", "generated_content(site_id, created_at DESC)"),
            
            # ìƒì„±ëœ ì œëª© ì¸ë±ìŠ¤
            ("idx_generated_titles_user_created", "generated_titles(user_id, created_at DESC)"),
            ("idx_generated_titles_keyword", "generated_titles(keyword_id)"),
            
            # í¬ìŠ¤íŒ… ê²°ê³¼ ì¸ë±ìŠ¤
            ("idx_posting_results_status", "posting_results(status, created_at DESC)"),
            ("idx_posting_results_site", "posting_results(site_id, created_at DESC)"),
            
            # ìë™í™” ì„¸ì…˜ ì¸ë±ìŠ¤
            ("idx_automation_sessions_user", "automation_sessions(user_id, created_at DESC)"),
            ("idx_automation_sessions_status", "automation_sessions(status)"),
        ]
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            for index_name, index_def in indexes:
                try:
                    start_time = time.time()
                    cursor.execute(f"CREATE INDEX IF NOT EXISTS {index_name} ON {index_def}")
                    elapsed = (time.time() - start_time) * 1000
                    
                    self.optimization_results.append({
                        "type": "index",
                        "name": index_name,
                        "time_ms": round(elapsed, 2),
                        "status": "created"
                    })
                    print(f"  âœ… {index_name} ìƒì„±ë¨ ({elapsed:.2f}ms)")
                except Exception as e:
                    self.optimization_results.append({
                        "type": "index",
                        "name": index_name,
                        "status": "failed",
                        "error": str(e)
                    })
                    print(f"  âŒ {index_name} ì‹¤íŒ¨: {e}")
            
            conn.commit()
    
    def analyze_slow_queries(self):
        """ëŠë¦° ì¿¼ë¦¬ ë¶„ì„"""
        print("\nğŸ” ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„...")
        
        test_queries = [
            # ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ë“¤
            ("ì‚¬ìš©ìë³„ ìµœê·¼ í‚¤ì›Œë“œ", """
                SELECT k.*, u.username 
                FROM keywords k 
                JOIN users u ON k.user_id = u.id 
                WHERE k.user_id = '1' 
                ORDER BY k.created_at DESC 
                LIMIT 10
            """),
            
            ("ì¸ê¸° í‚¤ì›Œë“œ ì¡°íšŒ", """
                SELECT keyword, COUNT(*) as usage_count, AVG(search_volume) as avg_volume
                FROM keywords 
                GROUP BY keyword 
                ORDER BY usage_count DESC 
                LIMIT 20
            """),
            
            ("ìµœê·¼ ìƒì„±ëœ ì½˜í…ì¸ ", """
                SELECT gc.*, gt.title 
                FROM generated_content gc
                LEFT JOIN generated_titles gt ON gc.title_id = gt.id
                ORDER BY gc.created_at DESC
                LIMIT 50
            """),
            
            ("ì‚¬ì´íŠ¸ë³„ í¬ìŠ¤íŒ… í†µê³„", """
                SELECT site_id, 
                       COUNT(*) as total_posts,
                       SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as success_count
                FROM posting_results
                GROUP BY site_id
            """)
        ]
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            for query_name, query in test_queries:
                try:
                    # EXPLAIN QUERY PLAN ì‹¤í–‰
                    cursor.execute(f"EXPLAIN QUERY PLAN {query}")
                    plan = cursor.fetchall()
                    
                    # ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
                    start_time = time.time()
                    cursor.execute(query)
                    results = cursor.fetchall()
                    elapsed = (time.time() - start_time) * 1000
                    
                    self.optimization_results.append({
                        "type": "query",
                        "name": query_name,
                        "time_ms": round(elapsed, 2),
                        "rows": len(results),
                        "plan": [dict(row) for row in plan]
                    })
                    
                    print(f"  ğŸ“Š {query_name}: {elapsed:.2f}ms ({len(results)} rows)")
                    
                except Exception as e:
                    print(f"  âŒ {query_name} ì‹¤íŒ¨: {e}")
    
    def optimize_table_structure(self):
        """í…Œì´ë¸” êµ¬ì¡° ìµœì í™”"""
        print("\nğŸ”§ í…Œì´ë¸” êµ¬ì¡° ìµœì í™”...")
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # VACUUMìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬
            start_time = time.time()
            cursor.execute("VACUUM")
            vacuum_time = (time.time() - start_time) * 1000
            print(f"  âœ… VACUUM ì™„ë£Œ ({vacuum_time:.2f}ms)")
            
            # ANALYZEë¡œ ì¿¼ë¦¬ í”Œë˜ë„ˆ ìµœì í™”
            start_time = time.time()
            cursor.execute("ANALYZE")
            analyze_time = (time.time() - start_time) * 1000
            print(f"  âœ… ANALYZE ì™„ë£Œ ({analyze_time:.2f}ms)")
            
            self.optimization_results.append({
                "type": "maintenance",
                "operations": {
                    "vacuum_ms": round(vacuum_time, 2),
                    "analyze_ms": round(analyze_time, 2)
                }
            })
    
    def create_materialized_views(self):
        """ìì£¼ ì‚¬ìš©ë˜ëŠ” ì§‘ê³„ ë°ì´í„°ë¥¼ ìœ„í•œ ë·° ìƒì„±"""
        print("\nğŸ“Š ì§‘ê³„ ë·° ìƒì„±...")
        
        views = [
            ("user_stats", """
                CREATE VIEW IF NOT EXISTS user_stats AS
                SELECT 
                    u.id as user_id,
                    u.username,
                    COUNT(DISTINCT k.id) as total_keywords,
                    COUNT(DISTINCT gc.id) as total_content,
                    COUNT(DISTINCT pr.id) as total_posts
                FROM users u
                LEFT JOIN keywords k ON u.id = k.user_id
                LEFT JOIN generated_content gc ON u.id = gc.user_id
                LEFT JOIN posting_results pr ON gc.id = pr.content_id
                GROUP BY u.id
            """),
            
            ("daily_stats", """
                CREATE VIEW IF NOT EXISTS daily_stats AS
                SELECT 
                    DATE(created_at) as date,
                    COUNT(CASE WHEN type = 'keyword' THEN 1 END) as keywords_created,
                    COUNT(CASE WHEN type = 'content' THEN 1 END) as content_created,
                    COUNT(CASE WHEN type = 'post' THEN 1 END) as posts_published
                FROM (
                    SELECT 'keyword' as type, created_at FROM keywords
                    UNION ALL
                    SELECT 'content' as type, created_at FROM generated_content
                    UNION ALL
                    SELECT 'post' as type, created_at FROM posting_results WHERE status = 'success'
                )
                GROUP BY DATE(created_at)
            """)
        ]
        
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            for view_name, view_sql in views:
                try:
                    start_time = time.time()
                    cursor.execute(view_sql)
                    elapsed = (time.time() - start_time) * 1000
                    
                    self.optimization_results.append({
                        "type": "view",
                        "name": view_name,
                        "time_ms": round(elapsed, 2),
                        "status": "created"
                    })
                    print(f"  âœ… {view_name} ë·° ìƒì„±ë¨ ({elapsed:.2f}ms)")
                except Exception as e:
                    print(f"  âŒ {view_name} ì‹¤íŒ¨: {e}")
            
            conn.commit()
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """ìµœì í™” ê²°ê³¼ ë³´ê³ ì„œ"""
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "results": self.optimization_results,
            "summary": {
                "indexes_created": len([r for r in self.optimization_results if r["type"] == "index" and r.get("status") == "created"]),
                "queries_analyzed": len([r for r in self.optimization_results if r["type"] == "query"]),
                "views_created": len([r for r in self.optimization_results if r["type"] == "view" and r.get("status") == "created"]),
            }
        }
        
        # í‰ê·  ì¿¼ë¦¬ ì‹œê°„
        query_times = [r["time_ms"] for r in self.optimization_results if r["type"] == "query" and "time_ms" in r]
        if query_times:
            report["summary"]["avg_query_time_ms"] = round(sum(query_times) / len(query_times), 2)
        
        return report
    
    def run_all_optimizations(self):
        """ëª¨ë“  ìµœì í™” ì‹¤í–‰"""
        print("ğŸš€ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹œì‘...")
        print("=" * 60)
        
        # 1. ì¸ë±ìŠ¤ ìƒì„±
        self.create_indexes()
        
        # 2. ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
        self.analyze_slow_queries()
        
        # 3. í…Œì´ë¸” êµ¬ì¡° ìµœì í™”
        self.optimize_table_structure()
        
        # 4. ì§‘ê³„ ë·° ìƒì„±
        self.create_materialized_views()
        
        # 5. ë³´ê³ ì„œ ì¶œë ¥
        report = self.get_optimization_report()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ìµœì í™” ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"  ì¸ë±ìŠ¤ ìƒì„±: {report['summary']['indexes_created']}ê°œ")
        print(f"  ì¿¼ë¦¬ ë¶„ì„: {report['summary']['queries_analyzed']}ê°œ")
        print(f"  ë·° ìƒì„±: {report['summary']['views_created']}ê°œ")
        
        if "avg_query_time_ms" in report["summary"]:
            print(f"  í‰ê·  ì¿¼ë¦¬ ì‹œê°„: {report['summary']['avg_query_time_ms']}ms")
        
        print("=" * 60)
        
        return report


# ë°°ì¹˜ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
class BatchProcessor:
    @staticmethod
    def batch_insert(conn, table: str, columns: List[str], data: List[tuple], batch_size: int = 1000):
        """ëŒ€ëŸ‰ ë°ì´í„° ì‚½ì… ìµœì í™”"""
        cursor = conn.cursor()
        placeholders = ",".join(["?" for _ in columns])
        insert_sql = f"INSERT INTO {table} ({','.join(columns)}) VALUES ({placeholders})"
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            cursor.executemany(insert_sql, batch)
        
        conn.commit()
    
    @staticmethod
    def batch_update(conn, table: str, updates: List[Dict[str, Any]], batch_size: int = 500):
        """ëŒ€ëŸ‰ ì—…ë°ì´íŠ¸ ìµœì í™”"""
        cursor = conn.cursor()
        
        for i in range(0, len(updates), batch_size):
            batch = updates[i:i + batch_size]
            
            for update in batch:
                set_clause = ", ".join([f"{k} = ?" for k in update["data"].keys()])
                where_clause = " AND ".join([f"{k} = ?" for k in update["where"].keys()])
                
                sql = f"UPDATE {table} SET {set_clause} WHERE {where_clause}"
                values = list(update["data"].values()) + list(update["where"].values())
                
                cursor.execute(sql, values)
            
            conn.commit()


if __name__ == "__main__":
    # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤í–‰
    db_path = "/mnt/e/project/test-blogauto-project/backend/blogauto_personal.db"
    optimizer = DatabaseOptimizer(db_path)
    optimizer.run_all_optimizations()