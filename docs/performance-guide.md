# BlogAuto ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ì„±ëŠ¥ ìµœì í™” ê°œìš”](#ì„±ëŠ¥-ìµœì í™”-ê°œìš”)
2. [ìºì‹± ì „ëµ](#ìºì‹±-ì „ëµ)
3. [ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”](#ë°ì´í„°ë² ì´ìŠ¤-ìµœì í™”)
4. [API ì„±ëŠ¥ ê°œì„ ](#api-ì„±ëŠ¥-ê°œì„ )
5. [í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™”](#í”„ë¡ íŠ¸ì—”ë“œ-ìµœì í™”)
6. [ì¸í”„ë¼ ìµœì í™”](#ì¸í”„ë¼-ìµœì í™”)
7. [ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§](#ëª¨ë‹ˆí„°ë§-ë°-í”„ë¡œíŒŒì¼ë§)
8. [ì„±ëŠ¥ í…ŒìŠ¤íŠ¸](#ì„±ëŠ¥-í…ŒìŠ¤íŠ¸)

## ğŸš€ ì„±ëŠ¥ ìµœì í™” ê°œìš”

BlogAutoëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„±ëŠ¥ ëª©í‘œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

- **API ì‘ë‹µ ì‹œê°„**: 95% ìš”ì²­ì´ 2ì´ˆ ì´ë‚´
- **ë™ì‹œ ì‚¬ìš©ì**: 1,000ëª… ì´ìƒ ì²˜ë¦¬
- **ì²˜ë¦¬ëŸ‰**: ì´ˆë‹¹ 100ê°œ ì´ìƒì˜ ìš”ì²­ ì²˜ë¦¬
- **ê°€ìš©ì„±**: 99.9% ì´ìƒ

## ğŸ’¾ ìºì‹± ì „ëµ

### 1. í•˜ì´ë¸Œë¦¬ë“œ ìºì‹± ì‹œìŠ¤í…œ

BlogAutoëŠ” L1(ë©”ëª¨ë¦¬) + L2(Redis) í•˜ì´ë¸Œë¦¬ë“œ ìºì‹±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
# ìºì‹± ì„¤ì • ì˜ˆì œ
from caching_system import cached, cache_manager

# ìºì‹± ë°ì½”ë ˆì´í„° ì‚¬ìš©
@cached(prefix="keywords", ttl=3600, compress=True)
async def analyze_keywords(keyword: str, country: str = "KR"):
    # ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì‘ì—…
    return expensive_operation(keyword, country)

# ìˆ˜ë™ ìºì‹œ ê´€ë¦¬
cache_key = "user_settings:123"
await cache_manager.set(cache_key, settings, ttl=7200)
settings = await cache_manager.get(cache_key)
```

### 2. ìºì‹± ì •ì±…

| ë°ì´í„° íƒ€ì… | TTL | ìºì‹œ ë ˆë²¨ | ì••ì¶• ì—¬ë¶€ |
|------------|-----|-----------|----------|
| í‚¤ì›Œë“œ ë¶„ì„ | 1ì‹œê°„ | L1 + L2 | Yes |
| ì œëª© ìƒì„± | 24ì‹œê°„ | L2 | No |
| ì½˜í…ì¸  ìƒì„± | 24ì‹œê°„ | L2 | Yes |
| API ì‘ë‹µ | 5ë¶„ | L1 + L2 | No |
| ì‚¬ìš©ì ì„¸ì…˜ | 1ì‹œê°„ | L1 + L2 | No |

### 3. ìºì‹œ ë¬´íš¨í™” ì „ëµ

```python
# íŒ¨í„´ ê¸°ë°˜ ìºì‹œ ì‚­ì œ
await cache_manager.clear_pattern("keywords:*")

# íŠ¹ì • ìºì‹œ ì‚­ì œ
await analyze_keywords.invalidate("python", "KR")

# ì „ì²´ ìºì‹œ í´ë¦¬ì–´ (ì£¼ì˜!)
await cache_manager.l1_cache.clear()
```

### 4. ìºì‹œ ì›Œë°

```python
# ì¸ê¸° í‚¤ì›Œë“œ ë¯¸ë¦¬ ë¡œë“œ
async def warm_popular_keywords():
    popular_keywords = ["ë¸”ë¡œê·¸", "ë§ˆì¼€íŒ…", "SEO", "ì½˜í…ì¸ "]
    
    for keyword in popular_keywords:
        await analyze_keywords(keyword, "KR")
    
    logger.info(f"Warmed {len(popular_keywords)} keywords")

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰
@app.on_event("startup")
async def startup_event():
    await cache_manager.initialize()
    await warm_popular_keywords()
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### 1. ì—°ê²° í’€ ì„¤ì •

```python
# SQLAlchemy ì—°ê²° í’€ ìµœì í™”
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,          # ê¸°ë³¸ í’€ í¬ê¸°
    max_overflow=10,       # ìµœëŒ€ ì˜¤ë²„í”Œë¡œìš°
    pool_timeout=30,       # ì—°ê²° ëŒ€ê¸° ì‹œê°„
    pool_recycle=3600,     # ì—°ê²° ì¬í™œìš© (1ì‹œê°„)
    pool_pre_ping=True,    # ì—°ê²° ìƒíƒœ í™•ì¸
)
```

### 2. ì¿¼ë¦¬ ìµœì í™”

```python
# íš¨ìœ¨ì ì¸ ì¿¼ë¦¬ ì‘ì„±
from sqlalchemy import select, func
from sqlalchemy.orm import selectinload, joinedload

# N+1 ë¬¸ì œ ë°©ì§€ - Eager Loading
stmt = select(BlogPost).options(
    joinedload(BlogPost.author),
    selectinload(BlogPost.tags)
).limit(20)

# ì§‘ê³„ ì¿¼ë¦¬ ìµœì í™”
keyword_stats = await session.execute(
    select(
        Keyword.name,
        func.count(KeywordAnalysis.id).label('analysis_count'),
        func.avg(KeywordAnalysis.search_volume).label('avg_volume')
    ).join(KeywordAnalysis)
    .group_by(Keyword.name)
    .having(func.count(KeywordAnalysis.id) > 10)
)

# ì¸ë±ìŠ¤ í™œìš©
# SQL: CREATE INDEX idx_created_at ON blog_posts(created_at DESC);
recent_posts = await session.execute(
    select(BlogPost)
    .order_by(BlogPost.created_at.desc())
    .limit(10)
)
```

### 3. ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤

```sql
-- ìì£¼ ì¡°íšŒë˜ëŠ” ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ì¶”ê°€
CREATE INDEX idx_keywords_name ON keywords(name);
CREATE INDEX idx_analysis_keyword_date ON keyword_analysis(keyword_id, created_at DESC);
CREATE INDEX idx_posts_status_date ON blog_posts(status, published_at DESC);

-- ë³µí•© ì¸ë±ìŠ¤ (WHERE ì ˆ ìˆœì„œì™€ ì¼ì¹˜)
CREATE INDEX idx_posts_author_status ON blog_posts(author_id, status);

-- ë¶€ë¶„ ì¸ë±ìŠ¤ (íŠ¹ì • ì¡°ê±´ë§Œ)
CREATE INDEX idx_active_users ON users(email) WHERE is_active = true;

-- ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
CREATE INDEX idx_content_search ON blog_posts USING gin(to_tsvector('korean', content));
```

### 4. ì¿¼ë¦¬ ë¶„ì„ ë° íŠœë‹

```sql
-- ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ë¶„ì„
EXPLAIN ANALYZE SELECT * FROM keyword_analysis 
WHERE keyword_id = 123 
ORDER BY created_at DESC 
LIMIT 10;

-- ëŠë¦° ì¿¼ë¦¬ ë¡œê¹… í™œì„±í™”
ALTER SYSTEM SET log_min_duration_statement = 1000; -- 1ì´ˆ ì´ìƒ
SELECT pg_reload_conf();

-- í…Œì´ë¸” í†µê³„ ì—…ë°ì´íŠ¸
ANALYZE keywords;
VACUUM ANALYZE keyword_analysis;
```

## âš¡ API ì„±ëŠ¥ ê°œì„ 

### 1. ë¹„ë™ê¸° ì²˜ë¦¬

```python
# ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
async def analyze_multiple_keywords(keywords: List[str]):
    tasks = [analyze_keywords(keyword) for keyword in keywords]
    results = await asyncio.gather(*tasks)
    return results

# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
from fastapi import BackgroundTasks

@app.post("/api/content/generate")
async def generate_content(
    request: ContentRequest,
    background_tasks: BackgroundTasks
):
    # ì¦‰ì‹œ ì‘ë‹µ
    task_id = str(uuid.uuid4())
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
    background_tasks.add_task(
        process_content_generation,
        task_id,
        request
    )
    
    return {"task_id": task_id, "status": "processing"}
```

### 2. ì‘ë‹µ ì••ì¶•

```python
# Gzip ì••ì¶• ë¯¸ë“¤ì›¨ì–´
from performance_optimizer import response_optimizer

app.middleware("http")(response_optimizer.compression_middleware)

# ì»¤ìŠ¤í…€ ì••ì¶• ì„¤ì •
response_optimizer.min_compress_size = 1024  # 1KB ì´ìƒë§Œ ì••ì¶•
response_optimizer.compress_level = 6        # ì••ì¶• ë ˆë²¨ (1-9)
```

### 3. í˜ì´ì§€ë„¤ì´ì…˜ ìµœì í™”

```python
# ì»¤ì„œ ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜
@app.get("/api/posts")
async def get_posts(
    cursor: Optional[str] = None,
    limit: int = Query(20, le=100)
):
    query = select(BlogPost).order_by(BlogPost.id.desc())
    
    if cursor:
        decoded_cursor = base64.b64decode(cursor).decode()
        query = query.where(BlogPost.id < int(decoded_cursor))
    
    posts = await session.execute(query.limit(limit + 1))
    posts = posts.scalars().all()
    
    has_next = len(posts) > limit
    if has_next:
        posts = posts[:-1]
    
    next_cursor = None
    if has_next and posts:
        next_cursor = base64.b64encode(
            str(posts[-1].id).encode()
        ).decode()
    
    return {
        "items": posts,
        "next_cursor": next_cursor,
        "has_next": has_next
    }
```

### 4. HTTP ì—°ê²° í’€ë§

```python
# aiohttp ì„¸ì…˜ ì¬ì‚¬ìš©
from performance_optimizer import http_pool

# ì´ˆê¸°í™”
await http_pool.initialize()

# API í˜¸ì¶œ ì‹œ ì—°ê²° ì¬ì‚¬ìš©
response = await http_pool.request(
    "POST",
    "https://api.openai.com/v1/chat/completions",
    json=payload,
    headers=headers
)

# ì—°ê²° í’€ ìƒíƒœ í™•ì¸
pool_status = http_pool.get_pool_status()
print(f"Active connections: {pool_status['connections']}")
```

## ğŸ¨ í”„ë¡ íŠ¸ì—”ë“œ ìµœì í™”

### 1. ë²ˆë“¤ ìµœì í™”

```javascript
// webpack.config.js
module.exports = {
  optimization: {
    splitChunks: {
      chunks: 'all',
      cacheGroups: {
        vendor: {
          test: /[\\/]node_modules[\\/]/,
          name: 'vendors',
          priority: 10
        },
        common: {
          minChunks: 2,
          priority: 5,
          reuseExistingChunk: true
        }
      }
    },
    minimizer: [
      new TerserPlugin({
        parallel: true,
        terserOptions: {
          compress: {
            drop_console: true,
          },
        },
      }),
    ],
  }
};
```

### 2. ì´ë¯¸ì§€ ìµœì í™”

```javascript
// Next.js ì´ë¯¸ì§€ ìµœì í™”
import Image from 'next/image';

<Image
  src="/images/hero.jpg"
  alt="Hero Image"
  width={1200}
  height={600}
  placeholder="blur"
  loading="lazy"
  quality={85}
/>

// ë°˜ì‘í˜• ì´ë¯¸ì§€
<picture>
  <source
    srcSet="/images/hero-mobile.webp"
    media="(max-width: 768px)"
    type="image/webp"
  />
  <source
    srcSet="/images/hero-desktop.webp"
    media="(min-width: 769px)"
    type="image/webp"
  />
  <img src="/images/hero.jpg" alt="Hero" />
</picture>
```

### 3. ì½”ë“œ ìŠ¤í”Œë¦¬íŒ…

```javascript
// ë™ì  ì„í¬íŠ¸
const DashboardComponent = lazy(() => import('./Dashboard'));

// ë¼ìš°íŠ¸ ê¸°ë°˜ ìŠ¤í”Œë¦¬íŒ…
const routes = [
  {
    path: '/dashboard',
    component: lazy(() => import('./pages/Dashboard'))
  },
  {
    path: '/analytics',
    component: lazy(() => import('./pages/Analytics'))
  }
];

// ì¡°ê±´ë¶€ ë¡œë”©
if (userRole === 'admin') {
  const AdminPanel = await import('./AdminPanel');
  AdminPanel.init();
}
```

### 4. ìƒíƒœ ê´€ë¦¬ ìµœì í™”

```javascript
// Redux Toolkit ìµœì í™”
import { createSlice, createSelector } from '@reduxjs/toolkit';

// ë©”ëª¨ì´ì œì´ì…˜ëœ ì…€ë ‰í„°
const selectPosts = state => state.posts.items;
const selectFilter = state => state.posts.filter;

export const selectFilteredPosts = createSelector(
  [selectPosts, selectFilter],
  (posts, filter) => {
    return posts.filter(post => 
      post.status === filter.status &&
      post.title.includes(filter.search)
    );
  }
);

// React.memoë¡œ ë¦¬ë Œë”ë§ ë°©ì§€
export const PostItem = React.memo(({ post, onEdit }) => {
  return (
    <div className="post-item">
      <h3>{post.title}</h3>
      <button onClick={() => onEdit(post.id)}>Edit</button>
    </div>
  );
}, (prevProps, nextProps) => {
  return prevProps.post.id === nextProps.post.id &&
         prevProps.post.updatedAt === nextProps.post.updatedAt;
});
```

## ğŸ—ï¸ ì¸í”„ë¼ ìµœì í™”

### 1. Docker ìµœì í™”

```dockerfile
# ë©€í‹° ìŠ¤í…Œì´ì§€ ë¹Œë“œ
FROM python:3.11-slim as builder

WORKDIR /app
COPY requirements.txt .
RUN pip install --user -r requirements.txt

FROM python:3.11-slim

# ë³´ì•ˆ ì‚¬ìš©ì
RUN useradd -m -u 1000 appuser

WORKDIR /app
COPY --from=builder /root/.local /home/appuser/.local
COPY --chown=appuser:appuser . .

USER appuser
ENV PATH=/home/appuser/.local/bin:$PATH

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8000/health')"

CMD ["gunicorn", "main:app", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker"]
```

### 2. Nginx íŠœë‹

```nginx
# nginx.conf ìµœì í™”
worker_processes auto;
worker_rlimit_nofile 65535;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    # íŒŒì¼ ìºì‹±
    open_file_cache max=10000 inactive=20s;
    open_file_cache_valid 30s;
    open_file_cache_min_uses 2;
    open_file_cache_errors on;

    # ë²„í¼ í¬ê¸° ìµœì í™”
    client_body_buffer_size 16K;
    client_header_buffer_size 1k;
    large_client_header_buffers 4 16k;
    client_max_body_size 50m;

    # Gzip ì••ì¶•
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss 
               application/x-font-ttf font/opentype image/svg+xml;

    # ì •ì  íŒŒì¼ ìºì‹±
    location ~* \.(jpg|jpeg|png|gif|ico|css|js|woff2)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
        add_header Vary "Accept-Encoding";
    }

    # API í”„ë¡ì‹œ ìºì‹±
    proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=api_cache:10m max_size=1g inactive=60m;
    
    location /api/keywords/suggestions {
        proxy_cache api_cache;
        proxy_cache_valid 200 10m;
        proxy_cache_key "$request_method$request_uri$args";
        add_header X-Cache-Status $upstream_cache_status;
        proxy_pass http://backend:8000;
    }
}
```

### 3. ì‹œìŠ¤í…œ íŠœë‹

```bash
# /etc/sysctl.conf ìµœì í™”
net.core.somaxconn = 65535
net.ipv4.tcp_max_tw_buckets = 1440000
net.ipv4.ip_local_port_range = 1024 65535
net.ipv4.tcp_fin_timeout = 15
net.ipv4.tcp_keepalive_time = 300
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.tcp_keepalive_intvl = 15

# íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ì œí•œ ì¦ê°€
echo "* soft nofile 65535" >> /etc/security/limits.conf
echo "* hard nofile 65535" >> /etc/security/limits.conf

# ì ìš©
sysctl -p
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

### 1. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì •ì˜
from prometheus_client import Counter, Histogram, Gauge

# ìš”ì²­ ì¹´ìš´í„°
request_count = Counter(
    'blogauto_requests_total',
    'Total number of requests',
    ['method', 'endpoint', 'status']
)

# ì‘ë‹µ ì‹œê°„ íˆìŠ¤í† ê·¸ë¨
request_duration = Histogram(
    'blogauto_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint'],
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

# í™œì„± ì—°ê²° ê²Œì´ì§€
active_connections = Gauge(
    'blogauto_active_connections',
    'Number of active connections'
)

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def collect_metrics(request: Request, call_next):
    start_time = time.time()
    
    # í™œì„± ì—°ê²° ì¦ê°€
    active_connections.inc()
    
    try:
        response = await call_next(request)
        duration = time.time() - start_time
        
        # ë©”íŠ¸ë¦­ ê¸°ë¡
        request_count.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).inc()
        
        request_duration.labels(
            method=request.method,
            endpoint=request.url.path
        ).observe(duration)
        
        return response
    finally:
        # í™œì„± ì—°ê²° ê°ì†Œ
        active_connections.dec()
```

### 2. í”„ë¡œíŒŒì¼ë§

```python
# cProfile ì‚¬ìš©
import cProfile
import pstats

def profile_function(func):
    def wrapper(*args, **kwargs):
        profiler = cProfile.Profile()
        profiler.enable()
        result = func(*args, **kwargs)
        profiler.disable()
        
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # ìƒìœ„ 20ê°œ í•¨ìˆ˜
        
        return result
    return wrapper

@profile_function
def expensive_operation():
    # í”„ë¡œíŒŒì¼ë§í•  ì½”ë“œ
    pass

# ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
from memory_profiler import profile

@profile
def memory_intensive_function():
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¶”ì í•  ì½”ë“œ
    large_list = [i for i in range(1000000)]
    return sum(large_list)
```

### 3. APM (Application Performance Monitoring)

```python
# Sentry Performance Monitoring
import sentry_sdk
from sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration

sentry_sdk.init(
    dsn=SENTRY_DSN,
    traces_sample_rate=0.1,  # 10% ìƒ˜í”Œë§
    profiles_sample_rate=0.1,  # 10% í”„ë¡œíŒŒì¼ë§
    integrations=[
        SqlalchemyIntegration(),
    ],
)

# íŠ¸ëœì­ì…˜ ì¶”ì 
with sentry_sdk.start_transaction(op="task", name="generate_content") as transaction:
    with transaction.start_child(op="db", description="fetch_keywords"):
        keywords = await fetch_keywords()
    
    with transaction.start_child(op="http", description="call_openai"):
        content = await generate_with_ai(keywords)
    
    return content
```

## ğŸ§ª ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### 1. ë¶€í•˜ í…ŒìŠ¤íŠ¸ (Locust)

```python
# locustfile.py
from locust import HttpUser, task, between

class BlogAutoUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        # ë¡œê·¸ì¸ ë˜ëŠ” ì´ˆê¸° ì„¤ì •
        self.client.post("/api/auth/login", json={
            "email": "test@example.com",
            "password": "testpass"
        })
    
    @task(3)
    def analyze_keyword(self):
        self.client.post("/api/keywords/analyze", json={
            "keyword": "python programming",
            "country": "KR"
        })
    
    @task(2)
    def generate_title(self):
        self.client.post("/api/titles/generate", json={
            "keyword": "web development",
            "count": 5
        })
    
    @task(1)
    def generate_content(self):
        self.client.post("/api/content/generate", json={
            "title": "10 Best Python Libraries",
            "keyword": "python libraries"
        })

# ì‹¤í–‰: locust -f locustfile.py --host=http://localhost:8000
```

### 2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```bash
# Apache Bench (ab)
ab -n 1000 -c 50 -T application/json -p request.json \
   http://localhost:8000/api/keywords/analyze

# wrk ì‚¬ìš©
wrk -t12 -c400 -d30s --latency \
    -s script.lua http://localhost:8000/api/health

# k6 ìŠ¤í¬ë¦½íŠ¸
import http from 'k6/http';
import { check } from 'k6';

export let options = {
  stages: [
    { duration: '2m', target: 100 },
    { duration: '5m', target: 100 },
    { duration: '2m', target: 200 },
    { duration: '5m', target: 200 },
    { duration: '2m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<2000'], // 95% ìš”ì²­ì´ 2ì´ˆ ì´ë‚´
    http_req_failed: ['rate<0.05'],    // ì—ëŸ¬ìœ¨ 5% ë¯¸ë§Œ
  },
};

export default function() {
  let response = http.get('http://localhost:8000/api/health');
  check(response, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
}
```

### 3. ì„±ëŠ¥ ëª©í‘œ ê²€ì¦

```python
# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìë™í™”
async def test_performance_requirements():
    results = {
        "api_response_time": await test_api_response_time(),
        "concurrent_users": await test_concurrent_users(),
        "throughput": await test_throughput(),
        "cache_hit_rate": await test_cache_performance()
    }
    
    # ê²€ì¦
    assert results["api_response_time"]["p95"] < 2.0, "API ì‘ë‹µ ì‹œê°„ ì´ˆê³¼"
    assert results["concurrent_users"]["max"] >= 1000, "ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬ ë¶€ì¡±"
    assert results["throughput"]["rps"] >= 100, "ì²˜ë¦¬ëŸ‰ ë¶€ì¡±"
    assert results["cache_hit_rate"] >= 0.8, "ìºì‹œ íˆíŠ¸ìœ¨ ë¶€ì¡±"
    
    return results
```

## ğŸ“ˆ ì„±ëŠ¥ ëª¨ë²” ì‚¬ë¡€

### 1. ì¼ë°˜ ì›ì¹™

- **ì¸¡ì • ë¨¼ì €**: ìµœì í™” ì „ì— í•­ìƒ ì¸¡ì •
- **ë³‘ëª© ì§€ì  ì°¾ê¸°**: í”„ë¡œíŒŒì¼ë§ìœ¼ë¡œ ì‹¤ì œ ë¬¸ì œ íŒŒì•…
- **ì ì§„ì  ê°œì„ **: í•œ ë²ˆì— í•˜ë‚˜ì”© ë³€ê²½
- **ëª¨ë‹ˆí„°ë§**: ë³€ê²½ í›„ ì˜í–¥ ì¶”ì 

### 2. ì½”ë“œ ë ˆë²¨

```python
# âŒ ë‚˜ìœ ì˜ˆ
for keyword in keywords:
    result = await analyze_keyword(keyword)  # ìˆœì°¨ ì²˜ë¦¬
    results.append(result)

# âœ… ì¢‹ì€ ì˜ˆ
results = await asyncio.gather(*[
    analyze_keyword(keyword) for keyword in keywords
])  # ë³‘ë ¬ ì²˜ë¦¬

# âŒ ë‚˜ìœ ì˜ˆ
users = session.query(User).all()
for user in users:
    posts = session.query(Post).filter_by(user_id=user.id).all()  # N+1

# âœ… ì¢‹ì€ ì˜ˆ
users = session.query(User).options(
    joinedload(User.posts)
).all()  # Eager loading
```

### 3. ì¸í”„ë¼ ë ˆë²¨

- **ìˆ˜í‰ í™•ì¥**: ë¡œë“œ ë°¸ëŸ°ì„œ + ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
- **CDN ì‚¬ìš©**: ì •ì  ìì› ë°°í¬
- **ì˜¤í† ìŠ¤ì¼€ì¼ë§**: íŠ¸ë˜í”½ì— ë”°ë¥¸ ìë™ í™•ì¥
- **ë°ì´í„°ë² ì´ìŠ¤ ë³µì œ**: ì½ê¸° ì „ìš© ë³µì œë³¸ ì‚¬ìš©

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [ìºì‹± ì‹œìŠ¤í…œ ë¬¸ì„œ](./caching-system.md)
- [ëª¨ë‹ˆí„°ë§ ê°€ì´ë“œ](./monitoring-guide.md)
- [ë°ì´í„°ë² ì´ìŠ¤ ê°€ì´ë“œ](./database-guide.md)
- [API ë¬¸ì„œ](./api-documentation.md)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025ë…„ 7ì›” 12ì¼  
**ë²„ì „**: v1.0.0  
**ë‹´ë‹¹ì**: BlogAuto ê°œë°œíŒ€