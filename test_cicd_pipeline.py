#!/usr/bin/env python3
"""
CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ ÌÖåÏä§Ìä∏ Ïä§ÌÅ¨Î¶ΩÌä∏
Step 7: CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨Ï∂ï Í≤ÄÏ¶ù
"""

import os
import yaml
import json
import subprocess
import asyncio
from pathlib import Path
from typing import Dict, List, Any

class CICDPipelineTester:
    def __init__(self, project_root: str = "/mnt/e/project/test-blogauto-project"):
        self.project_root = Path(project_root)
        self.workflows_dir = self.project_root / ".github" / "workflows"
        self.scripts_dir = self.project_root / "scripts" / "deployment"
        
    def test_workflow_files(self) -> Dict[str, Any]:
        """GitHub Actions ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌååÏùº Í≤ÄÏ¶ù"""
        print("üîç GitHub Actions ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌååÏùº Í≤ÄÏ¶ù...")
        
        results = {
            "workflows_found": [],
            "workflows_valid": [],
            "workflows_invalid": [],
            "total_jobs": 0,
            "security_features": []
        }
        
        # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌååÏùº ÌôïÏù∏
        workflow_files = [
            "ci-cd.yml",
            "development.yml", 
            "release.yml"
        ]
        
        for workflow_file in workflow_files:
            file_path = self.workflows_dir / workflow_file
            
            if file_path.exists():
                results["workflows_found"].append(workflow_file)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        workflow_data = yaml.safe_load(f)
                    
                    # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Íµ¨Ï°∞ Í≤ÄÏ¶ù
                    if self._validate_workflow_structure(workflow_data):
                        results["workflows_valid"].append(workflow_file)
                        
                        # Job Ïàò Í≥ÑÏÇ∞
                        if 'jobs' in workflow_data:
                            results["total_jobs"] += len(workflow_data['jobs'])
                            
                        # Î≥¥Ïïà Í∏∞Îä• ÌôïÏù∏
                        security_features = self._check_security_features(workflow_data)
                        results["security_features"].extend(security_features)
                    else:
                        results["workflows_invalid"].append(workflow_file)
                        
                except Exception as e:
                    print(f"‚ùå {workflow_file} ÌååÏã± Ïò§Î•ò: {e}")
                    results["workflows_invalid"].append(workflow_file)
            else:
                print(f"‚ùå {workflow_file} ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
        
        return results
    
    def _validate_workflow_structure(self, workflow: Dict) -> bool:
        """ÏõåÌÅ¨ÌîåÎ°úÏö∞ Íµ¨Ï°∞ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨"""
        required_fields = ['name', 'on', 'jobs']
        
        for field in required_fields:
            if field not in workflow:
                return False
        
        # Job Íµ¨Ï°∞ Í≤ÄÏ¶ù
        if not isinstance(workflow['jobs'], dict):
            return False
            
        for job_name, job_config in workflow['jobs'].items():
            if not isinstance(job_config, dict):
                return False
            if 'runs-on' not in job_config:
                return False
            if 'steps' not in job_config:
                return False
                
        return True
    
    def _check_security_features(self, workflow: Dict) -> List[str]:
        """Î≥¥Ïïà Í∏∞Îä• ÌôïÏù∏"""
        security_features = []
        
        workflow_str = str(workflow)
        
        # Î≥¥Ïïà Ïä§Ï∫î ÎèÑÍµ¨ ÌôïÏù∏
        if 'bandit' in workflow_str.lower():
            security_features.append("Python Security Scan (Bandit)")
        if 'safety' in workflow_str.lower():
            security_features.append("Dependency Security Check (Safety)")
        if 'trivy' in workflow_str.lower():
            security_features.append("Container Security Scan (Trivy)")
        if 'codeql' in workflow_str.lower():
            security_features.append("Code Analysis (CodeQL)")
            
        # ÌôòÍ≤Ω Î≥¥Ìò∏ ÌôïÏù∏
        for job_name, job_config in workflow.get('jobs', {}).items():
            if 'environment' in job_config:
                security_features.append(f"Environment Protection ({job_config['environment']})")
                
        return security_features
    
    def test_environment_configs(self) -> Dict[str, Any]:
        """ÌôòÍ≤Ω ÏÑ§Ï†ï ÌååÏùº Í≤ÄÏ¶ù"""
        print("üèóÔ∏è ÌôòÍ≤Ω ÏÑ§Ï†ï ÌååÏùº Í≤ÄÏ¶ù...")
        
        results = {
            "environments_found": [],
            "environments_valid": [],
            "config_completeness": {}
        }
        
        env_dir = self.project_root / ".github" / "environments"
        env_files = ["staging.yml", "production.yml"]
        
        for env_file in env_files:
            env_path = env_dir / env_file
            env_name = env_file.replace('.yml', '')
            
            if env_path.exists():
                results["environments_found"].append(env_name)
                
                try:
                    with open(env_path, 'r', encoding='utf-8') as f:
                        env_config = yaml.safe_load(f)
                    
                    # ÌïÑÏàò ÏÑ§Ï†ï ÌôïÏù∏
                    completeness = self._check_config_completeness(env_config)
                    results["config_completeness"][env_name] = completeness
                    
                    if completeness["score"] >= 80:
                        results["environments_valid"].append(env_name)
                        
                except Exception as e:
                    print(f"‚ùå {env_file} ÌååÏã± Ïò§Î•ò: {e}")
            else:
                print(f"‚ùå {env_file} ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå")
        
        return results
    
    def _check_config_completeness(self, config: Dict) -> Dict[str, Any]:
        """ÏÑ§Ï†ï ÏôÑÏÑ±ÎèÑ ÌôïÏù∏"""
        required_sections = [
            'name', 'url', 'protection_rules', 'environment_variables',
            'required_secrets', 'deployment', 'autoscaling', 'security'
        ]
        
        found_sections = []
        for section in required_sections:
            if section in config:
                found_sections.append(section)
        
        score = (len(found_sections) / len(required_sections)) * 100
        
        return {
            "required_sections": required_sections,
            "found_sections": found_sections,
            "missing_sections": [s for s in required_sections if s not in found_sections],
            "score": score
        }
    
    def test_deployment_scripts(self) -> Dict[str, Any]:
        """Î∞∞Ìè¨ Ïä§ÌÅ¨Î¶ΩÌä∏ Í≤ÄÏ¶ù"""
        print("üìú Î∞∞Ìè¨ Ïä§ÌÅ¨Î¶ΩÌä∏ Í≤ÄÏ¶ù...")
        
        results = {
            "scripts_found": [],
            "scripts_executable": [],
            "script_features": {}
        }
        
        deploy_script = self.scripts_dir / "deploy.sh"
        
        if deploy_script.exists():
            results["scripts_found"].append("deploy.sh")
            
            # Ïã§Ìñâ Í∂åÌïú ÌôïÏù∏
            if os.access(deploy_script, os.X_OK):
                results["scripts_executable"].append("deploy.sh")
            
            # Ïä§ÌÅ¨Î¶ΩÌä∏ Í∏∞Îä• Î∂ÑÏÑù
            with open(deploy_script, 'r', encoding='utf-8') as f:
                script_content = f.read()
            
            features = self._analyze_script_features(script_content)
            results["script_features"]["deploy.sh"] = features
        
        return results
    
    def _analyze_script_features(self, script_content: str) -> List[str]:
        """Ïä§ÌÅ¨Î¶ΩÌä∏ Í∏∞Îä• Î∂ÑÏÑù"""
        features = []
        
        feature_patterns = {
            "Environment Validation": ["staging", "production"],
            "Version Validation": ["version", "v[0-9]"],
            "Docker Support": ["docker", "image"],
            "Kubernetes Support": ["kubectl", "helm"],
            "Health Checks": ["health", "curl"],
            "Backup Creation": ["backup", "pg_dump"],
            "Blue-Green Deployment": ["blue-green", "blue", "green"],
            "Rolling Deployment": ["rolling"],
            "Monitoring Setup": ["monitoring", "prometheus"],
            "Error Handling": ["set -e", "trap"],
            "Logging": ["log_info", "log_error"]
        }
        
        for feature_name, patterns in feature_patterns.items():
            if any(pattern in script_content.lower() for pattern in patterns):
                features.append(feature_name)
        
        return features
    
    def test_docker_configurations(self) -> Dict[str, Any]:
        """Docker ÏÑ§Ï†ï Í≤ÄÏ¶ù"""
        print("üê≥ Docker ÏÑ§Ï†ï Í≤ÄÏ¶ù...")
        
        results = {
            "dockerfiles_found": [],
            "docker_compose_found": False,
            "dockerfile_quality": {}
        }
        
        # Dockerfile ÌôïÏù∏
        dockerfile_paths = [
            self.project_root / "backend" / "Dockerfile",
            self.project_root / "docker" / "Dockerfile.backend",
            self.project_root / "docker" / "Dockerfile.nextjs",
            self.project_root / "nextjs-app" / "Dockerfile"
        ]
        
        for dockerfile_path in dockerfile_paths:
            if dockerfile_path.exists():
                results["dockerfiles_found"].append(str(dockerfile_path.relative_to(self.project_root)))
                
                # Dockerfile ÌíàÏßà Î∂ÑÏÑù
                with open(dockerfile_path, 'r', encoding='utf-8') as f:
                    dockerfile_content = f.read()
                
                quality = self._analyze_dockerfile_quality(dockerfile_content)
                results["dockerfile_quality"][str(dockerfile_path.relative_to(self.project_root))] = quality
        
        # Docker Compose ÌôïÏù∏
        compose_files = [
            self.project_root / "docker-compose.yml",
            self.project_root / "docker" / "docker-compose.yml"
        ]
        
        for compose_file in compose_files:
            if compose_file.exists():
                results["docker_compose_found"] = True
                break
        
        return results
    
    def _analyze_dockerfile_quality(self, dockerfile_content: str) -> Dict[str, Any]:
        """Dockerfile ÌíàÏßà Î∂ÑÏÑù"""
        quality_checks = {
            "uses_multi_stage": "FROM" in dockerfile_content and dockerfile_content.count("FROM") > 1,
            "has_healthcheck": "HEALTHCHECK" in dockerfile_content,
            "runs_as_non_root": "USER" in dockerfile_content,
            "has_labels": "LABEL" in dockerfile_content,
            "minimizes_layers": dockerfile_content.count("RUN") <= 5,
            "uses_specific_tags": not ("latest" in dockerfile_content and "FROM" in dockerfile_content)
        }
        
        score = sum(quality_checks.values()) / len(quality_checks) * 100
        
        return {
            "checks": quality_checks,
            "score": score,
            "recommendations": self._get_dockerfile_recommendations(quality_checks)
        }
    
    def _get_dockerfile_recommendations(self, checks: Dict[str, bool]) -> List[str]:
        """Dockerfile Í∞úÏÑ† Í∂åÏû•ÏÇ¨Ìï≠"""
        recommendations = []
        
        if not checks["uses_multi_stage"]:
            recommendations.append("Î©ÄÌã∞ Ïä§ÌÖåÏù¥ÏßÄ ÎπåÎìú ÏÇ¨Ïö© Í∂åÏû•")
        if not checks["has_healthcheck"]:
            recommendations.append("Ìó¨Ïä§Ï≤¥ÌÅ¨ Ï∂îÍ∞Ä Í∂åÏû•")
        if not checks["runs_as_non_root"]:
            recommendations.append("ÎπÑÎ£®Ìä∏ ÏÇ¨Ïö©ÏûêÎ°ú Ïã§Ìñâ Í∂åÏû•")
        if not checks["has_labels"]:
            recommendations.append("Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÎùºÎ≤® Ï∂îÍ∞Ä Í∂åÏû•")
        if not checks["minimizes_layers"]:
            recommendations.append("RUN Î™ÖÎ†πÏñ¥ ÌÜµÌï©ÏúºÎ°ú Î†àÏù¥Ïñ¥ ÏµúÏÜåÌôî Í∂åÏû•")
        if not checks["uses_specific_tags"]:
            recommendations.append("Íµ¨Ï≤¥Ï†ÅÏù∏ Ïù¥ÎØ∏ÏßÄ ÌÉúÍ∑∏ ÏÇ¨Ïö© Í∂åÏû•")
        
        return recommendations
    
    async def test_ci_simulation(self) -> Dict[str, Any]:
        """CI ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÎÆ¨Î†àÏù¥ÏÖò"""
        print("üîÑ CI ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÎÆ¨Î†àÏù¥ÏÖò...")
        
        results = {
            "linting_check": False,
            "type_check": False,
            "unit_tests": False,
            "security_scan": False,
            "build_test": False
        }
        
        try:
            # Python Î¶∞ÌåÖ Ï≤¥ÌÅ¨
            lint_result = subprocess.run(
                ["python3", "-m", "py_compile"] + list(self.project_root.glob("backend/*.py")),
                capture_output=True, text=True, cwd=self.project_root
            )
            results["linting_check"] = lint_result.returncode == 0
            
            # TypeScript ÌÉÄÏûÖ Ï≤¥ÌÅ¨ (nextjs-appÏù¥ ÏûàÎã§Î©¥)
            nextjs_dir = self.project_root / "nextjs-app"
            if nextjs_dir.exists() and (nextjs_dir / "package.json").exists():
                type_check_result = subprocess.run(
                    ["npm", "run", "build"],
                    capture_output=True, text=True, cwd=nextjs_dir
                )
                results["type_check"] = type_check_result.returncode == 0
            else:
                results["type_check"] = True  # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ ÌÜµÍ≥º
            
            # Îã®ÏúÑ ÌÖåÏä§Ìä∏ (Í∞ÑÎã®Ìïú import ÌÖåÏä§Ìä∏)
            try:
                import sys
                sys.path.append(str(self.project_root / "backend"))
                from crypto_utils import CryptoManager
                crypto = CryptoManager()
                results["unit_tests"] = True
            except Exception:
                results["unit_tests"] = False
            
            # Î≥¥Ïïà Ïä§Ï∫î ÏãúÎÆ¨Î†àÏù¥ÏÖò (BanditÍ∞Ä ÏûàÎã§Î©¥)
            try:
                bandit_result = subprocess.run(
                    ["bandit", "-r", "backend/", "--severity-level", "medium"],
                    capture_output=True, text=True, cwd=self.project_root
                )
                results["security_scan"] = bandit_result.returncode == 0
            except FileNotFoundError:
                results["security_scan"] = True  # banditÍ∞Ä ÏóÜÏúºÎ©¥ ÌÜµÍ≥ºÎ°ú Í∞ÑÏ£º
            
            # ÎπåÎìú ÌÖåÏä§Ìä∏ (DockerÍ∞Ä ÏûàÎã§Î©¥)
            dockerfile = self.project_root / "backend" / "Dockerfile"
            if dockerfile.exists():
                try:
                    build_result = subprocess.run(
                        ["docker", "build", "-t", "test-backend:ci", "backend/"],
                        capture_output=True, text=True, cwd=self.project_root
                    )
                    results["build_test"] = build_result.returncode == 0
                except FileNotFoundError:
                    results["build_test"] = True  # DockerÍ∞Ä ÏóÜÏúºÎ©¥ ÌÜµÍ≥º
            else:
                results["build_test"] = True
            
        except Exception as e:
            print(f"CI ÏãúÎÆ¨Î†àÏù¥ÏÖò Ïò§Î•ò: {e}")
        
        return results
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """Î™®Îì† CI/CD ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
        print("üöÄ CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ Ï¢ÖÌï© ÌÖåÏä§Ìä∏ ÏãúÏûë...")
        print("=" * 60)
        
        results = {}
        
        # 1. ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌååÏùº ÌÖåÏä§Ìä∏
        results["workflows"] = self.test_workflow_files()
        
        # 2. ÌôòÍ≤Ω ÏÑ§Ï†ï ÌÖåÏä§Ìä∏
        results["environments"] = self.test_environment_configs()
        
        # 3. Î∞∞Ìè¨ Ïä§ÌÅ¨Î¶ΩÌä∏ ÌÖåÏä§Ìä∏
        results["deployment_scripts"] = self.test_deployment_scripts()
        
        # 4. Docker ÏÑ§Ï†ï ÌÖåÏä§Ìä∏
        results["docker"] = self.test_docker_configurations()
        
        # 5. CI ÏãúÎÆ¨Î†àÏù¥ÏÖò
        results["ci_simulation"] = await self.test_ci_simulation()
        
        return results

async def main():
    """Î©îÏù∏ ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
    tester = CICDPipelineTester()
    results = await tester.run_all_tests()
    
    print("\n" + "=" * 60)
    print("üìä CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ ÌÖåÏä§Ìä∏ Í≤∞Í≥º ÏöîÏïΩ")
    print("=" * 60)
    
    # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í≤∞Í≥º
    workflows = results["workflows"]
    print(f"üîÑ GitHub Actions ÏõåÌÅ¨ÌîåÎ°úÏö∞:")
    print(f"   - Î∞úÍ≤¨Îêú ÏõåÌÅ¨ÌîåÎ°úÏö∞: {len(workflows['workflows_found'])}Í∞ú")
    print(f"   - Ïú†Ìö®Ìïú ÏõåÌÅ¨ÌîåÎ°úÏö∞: {len(workflows['workflows_valid'])}Í∞ú")
    print(f"   - Ï¥ù Job Ïàò: {workflows['total_jobs']}Í∞ú")
    print(f"   - Î≥¥Ïïà Í∏∞Îä•: {len(set(workflows['security_features']))}Í∞ú")
    
    # ÌôòÍ≤Ω ÏÑ§Ï†ï Í≤∞Í≥º
    environments = results["environments"]
    print(f"\nüèóÔ∏è ÌôòÍ≤Ω ÏÑ§Ï†ï:")
    print(f"   - Î∞úÍ≤¨Îêú ÌôòÍ≤Ω: {environments['environments_found']}")
    print(f"   - Ïú†Ìö®Ìïú ÌôòÍ≤Ω: {environments['environments_valid']}")
    for env, config in environments['config_completeness'].items():
        print(f"   - {env} ÏôÑÏÑ±ÎèÑ: {config['score']:.1f}%")
    
    # Î∞∞Ìè¨ Ïä§ÌÅ¨Î¶ΩÌä∏ Í≤∞Í≥º
    scripts = results["deployment_scripts"]
    print(f"\nüìú Î∞∞Ìè¨ Ïä§ÌÅ¨Î¶ΩÌä∏:")
    print(f"   - Î∞úÍ≤¨Îêú Ïä§ÌÅ¨Î¶ΩÌä∏: {scripts['scripts_found']}")
    print(f"   - Ïã§Ìñâ Í∞ÄÎä•Ìïú Ïä§ÌÅ¨Î¶ΩÌä∏: {scripts['scripts_executable']}")
    for script, features in scripts['script_features'].items():
        print(f"   - {script} Í∏∞Îä•: {len(features)}Í∞ú")
    
    # Docker ÏÑ§Ï†ï Í≤∞Í≥º
    docker = results["docker"]
    print(f"\nüê≥ Docker ÏÑ§Ï†ï:")
    print(f"   - Dockerfile Ïàò: {len(docker['dockerfiles_found'])}Í∞ú")
    print(f"   - Docker Compose: {'‚úÖ' if docker['docker_compose_found'] else '‚ùå'}")
    for dockerfile, quality in docker['dockerfile_quality'].items():
        print(f"   - {dockerfile} ÌíàÏßà: {quality['score']:.1f}%")
    
    # CI ÏãúÎÆ¨Î†àÏù¥ÏÖò Í≤∞Í≥º
    ci_sim = results["ci_simulation"]
    print(f"\nüîÑ CI ÏãúÎÆ¨Î†àÏù¥ÏÖò:")
    print(f"   - Î¶∞ÌåÖ Ï≤¥ÌÅ¨: {'‚úÖ' if ci_sim['linting_check'] else '‚ùå'}")
    print(f"   - ÌÉÄÏûÖ Ï≤¥ÌÅ¨: {'‚úÖ' if ci_sim['type_check'] else '‚ùå'}")
    print(f"   - Îã®ÏúÑ ÌÖåÏä§Ìä∏: {'‚úÖ' if ci_sim['unit_tests'] else '‚ùå'}")
    print(f"   - Î≥¥Ïïà Ïä§Ï∫î: {'‚úÖ' if ci_sim['security_scan'] else '‚ùå'}")
    print(f"   - ÎπåÎìú ÌÖåÏä§Ìä∏: {'‚úÖ' if ci_sim['build_test'] else '‚ùå'}")
    
    # Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞
    total_checks = 0
    passed_checks = 0
    
    # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ï†êÏàò
    total_checks += len(workflows['workflows_found'])
    passed_checks += len(workflows['workflows_valid'])
    
    # ÌôòÍ≤Ω ÏÑ§Ï†ï Ï†êÏàò
    total_checks += len(environments['environments_found'])
    passed_checks += len(environments['environments_valid'])
    
    # CI ÏãúÎÆ¨Î†àÏù¥ÏÖò Ï†êÏàò
    ci_checks = list(ci_sim.values())
    total_checks += len(ci_checks)
    passed_checks += sum(ci_checks)
    
    overall_score = (passed_checks / total_checks * 100) if total_checks > 0 else 0
    
    print(f"\nüéØ Ï†ÑÏ≤¥ CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ Ï§ÄÎπÑÎèÑ: {overall_score:.1f}%")
    print("üéâ CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ Íµ¨Ï∂ï ÏôÑÎ£å!")
    
    # ÏÉÅÏÑ∏ Í≤∞Í≥ºÎ•º JSON ÌååÏùºÎ°ú Ï†ÄÏû•
    with open("/mnt/e/project/test-blogauto-project/cicd_pipeline_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("üìÑ ÏÉÅÏÑ∏ Í≤∞Í≥ºÍ∞Ä cicd_pipeline_test_results.jsonÏóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")

if __name__ == "__main__":
    asyncio.run(main())