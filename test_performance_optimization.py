#!/usr/bin/env python3
"""
ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Step 9: ì„±ëŠ¥ ìµœì í™” ë° ìºì‹± ì „ëµ ê²€ì¦
"""

import asyncio
import aiohttp
import time
import statistics
from typing import Dict, Any, List
import json

class PerformanceOptimizationTester:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def test_cache_performance(self) -> Dict[str, Any]:
        """ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ’¾ ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        test_keyword = "performance_test_cache"
        endpoint = "/api/keywords/analyze"
        data = {"keyword": test_keyword, "max_results": 5}
        
        # ì²« ë²ˆì§¸ ìš”ì²­ (ìºì‹œ ë¯¸ìŠ¤)
        start_time = time.time()
        async with self.session.post(f"{self.base_url}{endpoint}", json=data) as response:
            first_response_time = time.time() - start_time
            first_status = response.status
            first_data = await response.json() if response.status == 200 else None
        
        # ë‘ ë²ˆì§¸ ìš”ì²­ (ìºì‹œ íˆíŠ¸)
        start_time = time.time()
        async with self.session.post(f"{self.base_url}{endpoint}", json=data) as response:
            second_response_time = time.time() - start_time
            second_status = response.status
            second_data = await response.json() if response.status == 200 else None
        
        # ìºì‹œ ìƒíƒœ í™•ì¸
        async with self.session.get(f"{self.base_url}/api/performance/cache") as response:
            cache_status = await response.json() if response.status == 200 else {}
        
        # ìºì‹œ íš¨ê³¼ ê³„ì‚°
        if first_response_time > 0:
            speedup = first_response_time / second_response_time
        else:
            speedup = 0
        
        return {
            "cache_test": {
                "first_request": {
                    "response_time": first_response_time,
                    "status": first_status,
                    "cache_miss": True
                },
                "second_request": {
                    "response_time": second_response_time,
                    "status": second_status,
                    "cache_hit": second_response_time < first_response_time
                },
                "speedup": speedup,
                "cache_effective": speedup > 1.5
            },
            "cache_status": cache_status,
            "data_consistency": first_data == second_data if first_data and second_data else False
        }
    
    async def test_response_compression(self) -> Dict[str, Any]:
        """ì‘ë‹µ ì••ì¶• í…ŒìŠ¤íŠ¸"""
        print("ğŸ—œï¸ ì‘ë‹µ ì••ì¶• í…ŒìŠ¤íŠ¸...")
        
        # í° ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
        endpoint = "/api/keywords/analyze"
        data = {"keyword": "compression_test", "max_results": 20}
        
        # ì••ì¶• ì—†ì´ ìš”ì²­
        headers_no_compress = {"Accept-Encoding": "identity"}
        async with self.session.post(
            f"{self.base_url}{endpoint}", 
            json=data, 
            headers=headers_no_compress
        ) as response:
            uncompressed_size = len(await response.read())
            uncompressed_headers = dict(response.headers)
        
        # ì••ì¶• ìš”ì²­
        headers_compress = {"Accept-Encoding": "gzip"}
        async with self.session.post(
            f"{self.base_url}{endpoint}", 
            json=data, 
            headers=headers_compress
        ) as response:
            compressed_size = len(await response.read())
            compressed_headers = dict(response.headers)
            has_compression = response.headers.get("content-encoding") == "gzip"
        
        compression_ratio = 1 - (compressed_size / uncompressed_size) if uncompressed_size > 0 else 0
        
        return {
            "compression_enabled": has_compression,
            "uncompressed_size": uncompressed_size,
            "compressed_size": compressed_size,
            "compression_ratio": compression_ratio,
            "compression_effective": compression_ratio > 0.3,  # 30% ì´ìƒ ì••ì¶•
            "headers": {
                "uncompressed": uncompressed_headers.get("content-encoding", "none"),
                "compressed": compressed_headers.get("content-encoding", "none")
            }
        }
    
    async def test_concurrent_performance(self) -> Dict[str, Any]:
        """ë™ì‹œ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("ğŸ”„ ë™ì‹œ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        concurrent_levels = [1, 5, 10, 20]
        results = {}
        
        for level in concurrent_levels:
            response_times = await self._run_concurrent_requests(level)
            
            if response_times:
                results[f"concurrent_{level}"] = {
                    "count": len(response_times),
                    "avg_response_time": statistics.mean(response_times),
                    "min_response_time": min(response_times),
                    "max_response_time": max(response_times),
                    "p95_response_time": statistics.quantiles(response_times, n=20)[18],  # 95th percentile
                    "total_time": sum(response_times)
                }
            else:
                results[f"concurrent_{level}"] = {"error": "No successful requests"}
        
        # ìŠ¤ì¼€ì¼ë§ íš¨ìœ¨ì„± ê³„ì‚°
        if "concurrent_1" in results and "concurrent_10" in results:
            single_avg = results["concurrent_1"]["avg_response_time"]
            concurrent_avg = results["concurrent_10"]["avg_response_time"]
            scaling_efficiency = single_avg / concurrent_avg if concurrent_avg > 0 else 0
        else:
            scaling_efficiency = 0
        
        return {
            "concurrent_tests": results,
            "scaling_efficiency": scaling_efficiency,
            "scaling_good": scaling_efficiency > 0.7  # 70% ì´ìƒ ìœ ì§€
        }
    
    async def _run_concurrent_requests(self, count: int) -> List[float]:
        """ë™ì‹œ ìš”ì²­ ì‹¤í–‰"""
        async def make_request():
            start_time = time.time()
            try:
                async with self.session.post(
                    f"{self.base_url}/api/keywords/analyze",
                    json={"keyword": f"concurrent_test_{count}", "max_results": 3}
                ) as response:
                    await response.read()
                    return time.time() - start_time if response.status == 200 else None
            except:
                return None
        
        tasks = [make_request() for _ in range(count)]
        results = await asyncio.gather(*tasks)
        return [r for r in results if r is not None]
    
    async def test_connection_pooling(self) -> Dict[str, Any]:
        """ì—°ê²° í’€ë§ í…ŒìŠ¤íŠ¸"""
        print("ğŸŠ ì—°ê²° í’€ë§ í…ŒìŠ¤íŠ¸...")
        
        # HTTP ì—°ê²° í’€ ìƒíƒœ í™•ì¸
        async with self.session.get(f"{self.base_url}/api/performance/http-pool") as response:
            http_pool_status = await response.json() if response.status == 200 else {}
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ìƒíƒœ í™•ì¸
        async with self.session.get(f"{self.base_url}/api/performance/database") as response:
            db_status = await response.json() if response.status == 200 else {}
        
        # ì—¬ëŸ¬ ìš”ì²­ìœ¼ë¡œ ì—°ê²° í’€ í…ŒìŠ¤íŠ¸
        request_count = 50
        start_time = time.time()
        
        tasks = []
        for i in range(request_count):
            task = self.session.get(f"{self.base_url}/api/health")
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        successful_requests = sum(1 for r in responses if not isinstance(r, Exception) and r.status == 200)
        
        # ìš”ì²­ í›„ ì—°ê²° í’€ ìƒíƒœ
        async with self.session.get(f"{self.base_url}/api/performance/http-pool") as response:
            http_pool_after = await response.json() if response.status == 200 else {}
        
        return {
            "http_pool": {
                "before": http_pool_status,
                "after": http_pool_after,
                "connection_reuse": http_pool_status.get("connections", 0) > 0
            },
            "database_pool": db_status.get("connection_pool", {}),
            "stress_test": {
                "total_requests": request_count,
                "successful_requests": successful_requests,
                "total_time": total_time,
                "requests_per_second": request_count / total_time if total_time > 0 else 0
            }
        }
    
    async def test_performance_monitoring(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        print("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸...")
        
        # ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ì¡°íšŒ
        async with self.session.get(f"{self.base_url}/api/performance/summary") as response:
            if response.status == 200:
                performance_summary = await response.json()
                
                return {
                    "monitoring_active": True,
                    "metrics": {
                        "total_requests": performance_summary.get("total_requests", 0),
                        "requests_per_second": performance_summary.get("requests_per_second", 0),
                        "error_rate": performance_summary.get("error_rate", 0),
                        "slow_request_rate": performance_summary.get("slow_request_rate", 0)
                    },
                    "response_times": performance_summary.get("response_times", {}),
                    "slowest_endpoints": performance_summary.get("slowest_endpoints", [])[:5]
                }
            else:
                return {"monitoring_active": False, "error": f"HTTP {response.status}"}
    
    async def test_cache_invalidation(self) -> Dict[str, Any]:
        """ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸"""
        print("ğŸ—‘ï¸ ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸...")
        
        test_keyword = "invalidation_test"
        endpoint = "/api/keywords/analyze"
        data = {"keyword": test_keyword, "max_results": 3}
        
        # 1. ì²« ìš”ì²­ìœ¼ë¡œ ìºì‹œ ìƒì„±
        async with self.session.post(f"{self.base_url}{endpoint}", json=data) as response:
            original_data = await response.json() if response.status == 200 else None
        
        # 2. ìºì‹œ ìƒíƒœ í™•ì¸
        async with self.session.get(f"{self.base_url}/api/performance/cache") as response:
            cache_before = await response.json() if response.status == 200 else {}
        
        # 3. ìºì‹œ í´ë¦¬ì–´
        async with self.session.post(
            f"{self.base_url}/api/performance/cache/clear",
            params={"pattern": f"keywords:{test_keyword}*"}
        ) as response:
            clear_result = await response.json() if response.status == 200 else {}
        
        # 4. ìºì‹œ ìƒíƒœ ì¬í™•ì¸
        async with self.session.get(f"{self.base_url}/api/performance/cache") as response:
            cache_after = await response.json() if response.status == 200 else {}
        
        # 5. ë™ì¼ ìš”ì²­ìœ¼ë¡œ ìºì‹œ ë¯¸ìŠ¤ í™•ì¸
        start_time = time.time()
        async with self.session.post(f"{self.base_url}{endpoint}", json=data) as response:
            new_response_time = time.time() - start_time
            new_data = await response.json() if response.status == 200 else None
        
        return {
            "cache_invalidation_works": clear_result.get("success", False),
            "cleared_keys": clear_result.get("cleared_keys", 0),
            "cache_entries_before": cache_before.get("l1_cache", {}).get("entries", 0),
            "cache_entries_after": cache_after.get("l1_cache", {}).get("entries", 0),
            "response_time_after_clear": new_response_time,
            "data_consistency": original_data == new_data if original_data and new_data else False
        }
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        print("=" * 60)
        
        results = {}
        
        # 1. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        results["cache_performance"] = await self.test_cache_performance()
        
        # 2. ì‘ë‹µ ì••ì¶• í…ŒìŠ¤íŠ¸
        results["response_compression"] = await self.test_response_compression()
        
        # 3. ë™ì‹œ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        results["concurrent_performance"] = await self.test_concurrent_performance()
        
        # 4. ì—°ê²° í’€ë§ í…ŒìŠ¤íŠ¸
        results["connection_pooling"] = await self.test_connection_pooling()
        
        # 5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
        results["performance_monitoring"] = await self.test_performance_monitoring()
        
        # 6. ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸
        results["cache_invalidation"] = await self.test_cache_invalidation()
        
        return results

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    async with PerformanceOptimizationTester() as tester:
        results = await tester.run_all_tests()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        # ìºì‹œ ì„±ëŠ¥ ê²°ê³¼
        cache = results["cache_performance"]["cache_test"]
        print(f"ğŸ’¾ ìºì‹œ ì„±ëŠ¥:")
        print(f"   - ìºì‹œ íš¨ê³¼: {'âœ…' if cache['cache_effective'] else 'âŒ'}")
        print(f"   - ì†ë„ í–¥ìƒ: {cache['speedup']:.2f}ë°°")
        print(f"   - ì²« ìš”ì²­: {cache['first_request']['response_time']:.3f}ì´ˆ")
        print(f"   - ìºì‹œ ìš”ì²­: {cache['second_request']['response_time']:.3f}ì´ˆ")
        
        # ì‘ë‹µ ì••ì¶• ê²°ê³¼
        compression = results["response_compression"]
        print(f"\nğŸ—œï¸ ì‘ë‹µ ì••ì¶•:")
        print(f"   - ì••ì¶• í™œì„±í™”: {'âœ…' if compression['compression_enabled'] else 'âŒ'}")
        print(f"   - ì••ì¶•ë¥ : {compression['compression_ratio']:.1%}")
        print(f"   - ì›ë³¸ í¬ê¸°: {compression['uncompressed_size']} bytes")
        print(f"   - ì••ì¶• í¬ê¸°: {compression['compressed_size']} bytes")
        
        # ë™ì‹œ ìš”ì²­ ì„±ëŠ¥
        concurrent = results["concurrent_performance"]
        print(f"\nğŸ”„ ë™ì‹œ ìš”ì²­ ì„±ëŠ¥:")
        print(f"   - ìŠ¤ì¼€ì¼ë§ íš¨ìœ¨: {concurrent['scaling_efficiency']:.2f}")
        print(f"   - ìŠ¤ì¼€ì¼ë§ ìƒíƒœ: {'âœ… ì–‘í˜¸' if concurrent['scaling_good'] else 'âš ï¸ ê°œì„  í•„ìš”'}")
        if "concurrent_10" in concurrent["concurrent_tests"]:
            stats = concurrent["concurrent_tests"]["concurrent_10"]
            print(f"   - 10ê°œ ë™ì‹œ ìš”ì²­ í‰ê· : {stats['avg_response_time']:.3f}ì´ˆ")
            print(f"   - 95th percentile: {stats['p95_response_time']:.3f}ì´ˆ")
        
        # ì—°ê²° í’€ë§
        pooling = results["connection_pooling"]
        stress = pooling["stress_test"]
        print(f"\nğŸŠ ì—°ê²° í’€ë§:")
        print(f"   - HTTP ì—°ê²° ì¬ì‚¬ìš©: {'âœ…' if pooling['http_pool']['connection_reuse'] else 'âŒ'}")
        print(f"   - ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: {stress['successful_requests']}/{stress['total_requests']} ì„±ê³µ")
        print(f"   - ì²˜ë¦¬ëŸ‰: {stress['requests_per_second']:.1f} req/s")
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        monitoring = results["performance_monitoring"]
        if monitoring["monitoring_active"]:
            metrics = monitoring["metrics"]
            print(f"\nğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§:")
            print(f"   - ëª¨ë‹ˆí„°ë§ í™œì„±í™”: âœ…")
            print(f"   - ì´ ìš”ì²­ ìˆ˜: {metrics['total_requests']}")
            print(f"   - ì—ëŸ¬ìœ¨: {metrics['error_rate']:.1%}")
            print(f"   - ëŠë¦° ìš”ì²­ ë¹„ìœ¨: {metrics['slow_request_rate']:.1%}")
        
        # ìºì‹œ ë¬´íš¨í™”
        invalidation = results["cache_invalidation"]
        print(f"\nğŸ—‘ï¸ ìºì‹œ ë¬´íš¨í™”:")
        print(f"   - ë¬´íš¨í™” ì‘ë™: {'âœ…' if invalidation['cache_invalidation_works'] else 'âŒ'}")
        print(f"   - ì‚­ì œëœ í‚¤: {invalidation['cleared_keys']}")
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        total_checks = 6
        passed_checks = sum([
            cache["cache_effective"],
            compression["compression_effective"],
            concurrent["scaling_good"],
            pooling["http_pool"]["connection_reuse"],
            monitoring["monitoring_active"],
            invalidation["cache_invalidation_works"]
        ])
        
        success_rate = (passed_checks / total_checks) * 100
        
        print(f"\nğŸ¯ ì „ì²´ ì„±ëŠ¥ ìµœì í™” ì ìˆ˜: {success_rate:.1f}%")
        print("ğŸ‰ ì„±ëŠ¥ ìµœì í™” ë° ìºì‹± ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
        
        # ìƒì„¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        with open("/mnt/e/project/test-blogauto-project/performance_optimization_test_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print("ğŸ“„ ìƒì„¸ ê²°ê³¼ê°€ performance_optimization_test_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())